mlflow link: https://dagshub.com/azhgh22/ML01_House-Prices.mlflow/#/experiments/1?searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D

Experiment 1:
    steps:
        1.  გავაქრე ისეთი სვეტები, სადაც NA იების რაოდენობა/სტრიქონების რაოდენობაზე threshold-ზე მეტია, რაგდან ასეთი 
            feature-ები დიდი ალბათობით არანაირ(მნიშვნელოვან) ინფორმაციას არ შეიცავს

        grid_search tested thresholds: [0.6,0.7,0.8,0.9]

        2. დანარჩენ სვეტებს, სადაც NA-ების რაოდენობა ნაკლებია. მათ ვავსებთ მოდით.

        3. კატეგორიულების გადაყვანისას ვიყენებ OrdinalEncoder-ს და OneHostEncoder კლასებს.
            თუ კატეგორიების რაოდენობა სვეტში კონკრეტულ ზღვარზე მეტია ვიყენებ Ordinal-ს, წინააღმდეგ შემთსვავაში OneHotEncoding-ს

        grid_search tested thresholds: [7,10,20,30,50,100]

        4. correlation filtering: ამის შემდეგ კორელაციის ფილტრით გამოვრიცხავ ყველაზე მეტად კოლერილებული წყვილებში   
            (რომელთა კორელაციის კოეფიციენტი threshold-ზე მეტია),
             თარგეთთან ყველაზე ნაკლებად კოლერილებულ features-ს

        grid_search tested thresholds: [0.6,0.7,0.8,0.9,0.95,0.98]

        6. Grid Search - ში ვცდი Scaler-ის სხვა და სხვა ვარიანტს, საუკეთესო აღმოჩნდა RobustScaler()

            grid_search tested scalers: [MinMaxScaler(), RobustScaler(), None]

        5. LinearRegression გამოიყენება მოდელად.

        7. ვიყენებ Kfold cv

        რადგან მოდელის ეს კლასი Noraml Forms იყენებს იტერაციული gradient descent-ის მაგივრად,
        ის პირდაპირ მინიმალურ მნიშვნელობის შესაბამის თეტა ვექტორს დააბრუნებს.

    analyzes:
        grid search neg_root_mean_squared_error-ის მიხედვით არჩევს საუკეთესო მოდელს.

        grid search-ის შედეგებიდან გამომდინარე საუკეთესო მოდელის ჰიპერპარამეტრები ასეთია:
            dropna threshold = 50%
            cat2num threshold = 30
            corr filter threshold = 0.7
            scaler  = RobustScaler()

        უფრო დაბალი ან უფრო მაღალი ვარიანტების განხილვას თითოეული ჰიპერპარამეტრისთვის არ აქვს აზრი რადგან ერრორი ან 
        იზრდება ან იგივე რჩებოდა

        საუკეთესო მოდელის შედეგები:

        mean Dropped Features 47

        train rmse: 19695.8894
        test rmse: 35636.2346
        
        train r2: 0.9349
        test r2: 0.7724
        
        train mae: 13312.4908
        test mae: 18927.2100

        როგორც ჩანს გვაქვს მაღალი ვარიაცია, რადგან თითოეული შემფასებლის შემთხვევაში test და train ერრორები საკმაოდ დაშორებული ერთმანეთისგან.
        ეს ნიშნავს რომ მოდელმა დაიზეპირა train set, ზედმეტი მნიშვნელობა მიანიჭა outlayer-ებს. აქედან გამომდინარე კარგად ვერ განზოგადდა.
        გვაქვს overfit

Experiment 2:
    steps:
        ყველაფერი იგივე, ამ შემთხვევაში ვიყენებთ L2 რეგულარიზაციას, ვარიაციის შესამცირებლად
        tested_alphas : [0,10, 40, 70 ,100,1000,10000]

        და RFE -ს feature-ების რაოდენობის ესამცირებლად
        [40, 55 ,70, 85 ,100]
        
        alpha=10 და rfe_features_to_left=70 აჩვენა საუკეთესო შედეგი

    scores:
        mean_test_mae	mean_train_mae	mean_test_r2	mean_train_r2	mean_test_rmse	mean_train_rmse
        18810.506566	17471.280034	0.821044	    0.855494	    32400.224537	29329.642447

    analyzes:
        alpha რაც უფრო მცირდება მცირდება bias, მაგრამ იზრდება ვარიაცია. უფრო დიდ ალფებისთვის კი ბაიასი საკმაოდ დიდია, რადგან 
        ტესტ და ტრენინგ ერრორები ერთმანეთის ტოლია და ერორიც ძალიან გაიზარდა ბაიასის ხარჯზე

        ამ შედეგებიდან, ხვედავთ რომ საშუალოდ 18000 -ით არის აცდენილი ფასი ნამდვილ ფასს. დიდი ალბათობით იმიტომ რომ წრფივი რეგრესია
        საკმარისად კომლექსური მოდელი არ არის დამოკიდებულების აღსაწერად

Experiment 3:
    ამ ექსპერიმენტში ისევ წრფივ რეგრესიას ვიყენებ რეგულარიზაცცით. preprocessing-ის მხრვივ გაუმჯობესდა ბევრი რამ.
    1. სატრენინგო დატიდან ამოვაგდე outlayer სტრიქონები target-ის განაწილებაზე დაკვირვებით,
    2. თითული feature-ის განაწილებაზე დაკვირვებით ამოვაგდე ძალიან დეტერმინისტული(low variance) featereb-ი 
       ამოვარდა 23 feaure ამ სთეფზე ისე რომ ერრორი წინა run-თან შედარებით გაუმჯობესდა.
    3. კატეგორიულების გადაყვანისას თითოელი X-ის კატეგორიულისთვის ვითვლი ამ კატეგროიულის შესაბამისი y-ების საშუალოს.
       ეს გზა კარგია რადგან ნარჩუნდება იერარქია(importance) კატეგორიულ ცვლადებს შორის

    თუმცა grid-search -დან გამომდინარე ყველაფერზე one-hot -ის მოდებამ ბევრად უკეთესი შედეგი აჩვენა :D


ეს არის რეგულარიზაცცის კონსტანტის შერჩევა ყველა ამ პრეპროცესინგის გათვალისწინებით.

    params	mean_test_mae	mean_train_mae	mean_test_r2	mean_train_r2	mean_test_rmse	mean_train_rmse
0	{'Model__alpha': 10}	-16008.043991	-13966.654296	0.816380	0.882715	-25039.188150	-20401.325758
1	{'Model__alpha': 40}	-16293.520203	-14828.694915	0.812066	0.866131	-25311.826295	-21794.285698
2	{'Model__alpha': 60}	-16475.139281	-15190.650707	0.809608	0.859086	-25492.552044	-22359.900805
3	{'Model__alpha': 100}	-16783.845469	-15709.446758	0.805518	0.848647	-25801.548370	-23173.368198
4	{'Model__alpha': 500}	-18925.890284	-18475.456121	0.769974	0.792673	-28293.186046	-27130.815197
5	{'Model__alpha': 1000}	-21381.516052	-21143.799570	0.726155	0.741371	-30987.329397	-30308.566014


შედეგებინდა ჩანს, რომ წინა ექპერიმენთან შედარებით შედეგი საკმაოდ უკეთესია. პატარა alpha-სთვის ნორმალური შედეგია, თუმცა
სხვებთან შედარებით დიდი ვარიაცია. დიდი alpha-ებისთვვის ბაიაისი მატუოლობ (ტერინინგი და ტესტი ერთმანეთს ემსგავსება, ანუ
 არ ავს მნიშვნებლობა მოდლისთვის რომელ წერტილს მუვცემთ). ბაიასის ხარჯზე გაიზარდა ერორიც.

 მართალია alpha=10-სთვის ყველაზე კარგი შედეგი მივიღეთ, თუმცა იმის გამო რომ alpha=100 არც ისე განსხვავებული შედეგი აქვს და ვარიაციაც ნაკლები. ამ მოდელის ჰიპერპარამეტრად 100 დავტოვე.


Experiment 4
    Id feature-ის ამოგდებამ ხის პერფორმანსი გააუმჯობესა, მცირედით ანუ ეს ცვლადი არ ყოფილა საჭირო

    ამ ექსპერიმენტში მოდელად ვიყენებ რეგრესსის ხეს. feature_engineering-ის მხრივ არაფერი შეცვლილა.
    როგორც მოსალოდნელი იყო ამ მოდელმა მოგვცა დიდი ვარიაცია, წინა ექპერიმენტებთან შედარებით.
    
    run1:
        ვარიაციის შემცირებას ვცდილობ :
            max_depth, min_sample_split, min_impurity_decrease - ის მორგებით

        საუკთესო შედეგები:
        max_depth = 35
        min_impurity_decrease = 20
        min_sample_split = 25
    
                        params	        mean_test_mae	mean_train_mae	mean_test_r2	mean_train_r2	mean_test_rmse	mean_train_rmse
3	{'Model__min_samples_split': 25}	-21492.465089	-13596.368887	0.744636	    0.899698	    -29989.319110	-18858.005180
2	{'Model__min_samples_split': 20}	-21886.450859	-12095.543734	0.735631	    0.918008	    -30510.923588	-17053.710983
4	{'Model__min_samples_split': 30}	-21912.534584	-14799.940788	0.728808	    0.883996	    -30883.734700	-20288.574454
5	{'Model__min_samples_split': 40}	-22460.218757	-16529.135729	0.725132	    0.853642	    -31102.555605	-22783.133728
1	{'Model__min_samples_split': 15}	-22526.661012	-10392.416874	0.718447	    0.937502	    -31482.759173	-14897.466852
0	{'Model__min_samples_split': 10}	-22885.332049	-7739.013604	0.706310	    0.962711	    -32168.503280	-11478.082015
6	{'Model__min_samples_split': 100}	-23745.066724	-20963.828386	0.701540	    0.774755	    -32403.511395	-28280.988972

    ამ მაგალითიდან როგორც ჩანს, რაც უფრო დაბალია თითიოეული ეს პარმეტრი, მით დიდი ვარიაცია, რადაგნ ხე სივრცეს ყოფს ისე რომ იზეპირებს
    თითოეულ წერტილს, შედეგად ტესტზე ცუდი შედეგია.
    რა უფრო დიდი პარამეტრი, უფრო და უფრო ცოტა ნაწილად არის დაყოფილი, შედეგად გვაქვს დიდი ბაიასი, ერორიც ამის ხარჯზე იზრდება


    run2: 
        ვარიაციის შემცირებას ვცდილობ prooning-ის დახმარებით, დიდი განსხვავება არ ქონია:
    
    
    cpp_alpha    mean_test_mae	mean_train_mae	mean_test_r2	mean_train_r2	mean_test_rmse	mean_train_rmse
    30000	     -21577.527682	-13596.368887	0.742631	    0.899698	    -30105.262787	-18858.00518


Experiment 5
    რეგრესიის ხე შეიძლება იყო საკმარისად საკმარისად კომპლექსური იმისთვის რომ, აღწეროს სატრენინგო დატა, მაგრამ არის მაღალი ვარიაციის საშოშროება
    ვარიაციის შესამცირებლად გამოვიყენებთ bagging-ის ერთ-ერთ სახეს RandomForests-ს

    თუმცა ამ მცდელობამ შედეგი ვერ გამოიღო :D. დიდი ვარიაციაა. Overfit
    train rmse: 10925.2344
    test rmse: 22394.6101

    train r2: 0.9664
    test r2: 0.8577
    
    train mae: 7546.9768
    test mae: 15508.9544
სავარაუდოდ იმიტომ რომ პატარა dataset გვაქვს.
